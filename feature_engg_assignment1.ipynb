{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf756a16-65c8-4210-805a-a88173297b6e",
   "metadata": {},
   "source": [
    "#                                   ANSWER 1 \n",
    "In this method, features are filtered based on general characteristics (some metric such as correlation) of the dataset such correlation with the dependent variable. Filter method is performed without any predictive model. It is faster and usually the better approach when the number of features are huge. Avoids overfitting but sometimes may fail to select best features.\n",
    "\n",
    "Filter method is faster and useful when there are more number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f38d40-59b0-4832-8dd1-d15d0f77cb41",
   "metadata": {},
   "source": [
    "#                                   ANSWER 2\n",
    "The main differences between the filter and wrapper methods for feature selection are:\n",
    "\n",
    "Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "\n",
    "Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation.\n",
    "\n",
    "Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "\n",
    "Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d8bc2-18f7-4c47-914c-5dccf179cb8a",
   "metadata": {},
   "source": [
    "#                                   ANSWER 3\n",
    "decision tree-based algorithms (e.g., decision tree, random forest, gradient boosting), and feature selection using regularization models (e.g., LASSO or elastic net)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8cd2ac-80b8-42d2-85d0-d5b251999c1e",
   "metadata": {},
   "source": [
    "#                                   ANSWER 4\n",
    "\n",
    "Do not remove multicollinearity;\n",
    "\n",
    "Sometimes may fail in selection;\n",
    "\n",
    "they ignore the interaction with the classifier and each feature is considered independently thus ignoring feature dependencies In addition, it is not clear how to determine the threshold point for rankings to select only the required features and exclude noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5675c-4494-4b83-875e-b5828485ceb6",
   "metadata": {},
   "source": [
    "#                                   ANSWER 5\n",
    "For large data should use the Filter approaches because these approaches are rapid and for small size of data it is better to use Wrapper approaches because they are slower than the Filter approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34c4fd-4287-485f-9139-db5e12c65951",
   "metadata": {},
   "source": [
    "#                                   ANSWER 6\n",
    "## Choosing Features for Customer Churn Prediction using the Filter Method:\n",
    "1. Data Preprocessing: First, clean and preprocess the dataset by handling missing values, encoding categorical variables, and scaling numerical features if necessary.\n",
    "2. Correlation Analysis:Calculate the correlation between each feature and the target variable (customer churn). Features with higher correlation values are generally more relevant for predicting churn.\n",
    "You can use statistical metrics like Pearson correlation coefficient or rank correlation methods like Spearman or Kendall to quantify the relationship.\n",
    "3. Univariate Feature Selection:Conduct univariate statistical tests (e.g., chi-square test for categorical features or ANOVA for numerical features) to evaluate the relationship between each feature and the target variable independently.\n",
    "Select features with high statistical significance (low p-values) as they are more likely to be relevant for the model.\n",
    "4. Feature Importance Ranking:Utilize techniques like mutual information or information gain to rank features based on their individual predictive power for the target variable.\n",
    "5. Selecting Top Features:Combine the results from the above steps to create a ranking of features based on their relevance to predicting customer churn.\n",
    "Choose the top 'k' features that are most relevant for your model. The value of 'k' can be determined based on experimentation and performance evaluation.\n",
    "6. Build the Model:finally, use the selected top 'k' features to train your predictive model. Employ machine learning algorithms like logistic regression, decision trees, or random forests, and evaluate the model's performance on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78fdda-9582-4e67-8410-d8b1c9c4b647",
   "metadata": {},
   "source": [
    "# ANSWER 7 \n",
    "## Selecting Relevant Features for Soccer Match Outcome Prediction using the Embedded Method:\n",
    "1. Data Preprocessing:As usual, start by cleaning and preprocessing the dataset, handling any missing values, encoding categorical variables, and scaling numerical features if required.\n",
    "2. Feature Importance from Models:Choose a machine learning algorithm suitable for predicting soccer match outcomes, such as logistic regression, random forests, gradient boosting, or support vector machines.\n",
    "Train the model on the entire dataset, and then extract the feature importance or coefficients of the trained model.\n",
    "Some algorithms, like tree-based models, have inherent feature importance scores that can be directly accessed.\n",
    "3. Feature Selection:Based on the feature importance scores obtained from the model, select the most relevant features with higher importance values.\n",
    "You can set a threshold for feature importance or choose the top 'k' features, depending on the level of feature reduction desired.\n",
    "4. Iterative Process:In some cases, you might want to perform a recursive or iterative process. Train the model with the selected features, evaluate its performance, and iterate by removing less relevant features and retraining the model until a satisfactory result is achieved.\n",
    "5. Model Evaluation:After selecting the relevant features, evaluate the model's performance using appropriate metrics like accuracy, precision, recall, F1-score, or area under the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae8724-d1c0-4136-88ba-ee3fb8e4b60c",
   "metadata": {},
   "source": [
    "# ANSWER 8\n",
    "## Using the Wrapper Method for House Price Prediction:\n",
    "1. Data Preprocessing:As always, start by cleaning and preprocessing the dataset, handling missing values, encoding categorical variables, and scaling numerical features as necessary.\n",
    "2. Feature Subset Generation:Generate all possible combinations of the limited number of features available. For instance, if you have features A, B, and C, the subsets can be {A}, {B}, {C}, {A, B}, {A, C}, {B, C}, {A, B, C}. \n",
    "3. Model Training and Evaluation:For each feature subset, train a predictive model (e.g., linear regression, random forests, or any other appropriate regression algorithm) using the selected features.\n",
    "Evaluate the model's performance using a suitable evaluation metric (e.g., mean squared error or mean absolute error) on a validation set (or using cross-validation).\n",
    "4. Selecting the Best Subset: Compare the performance of each model trained on different feature subsets.\n",
    "Choose the feature subset that yields the best performance metric. This subset of features will be the selected set for your house price prediction model.\n",
    "5. Optional Refinement: Depending on computational resources and time constraints, you can further optimize the process by exploring advanced techniques like forward selection, backward elimination, or stepwise selection to find the best feature subset more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc076c35-b880-4389-80a3-628cc90c452c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d229f03-4206-41d9-b797-a7ac5cd4184f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a597e3a-3af3-4630-9d32-8adfa215a632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
